# ğŸ“Š Week 2: æœºå™¨å­¦ä¹ å»ºæ¨¡ï¼ˆROASé¢„æµ‹ï¼‰

> çŠ¶æ€ï¼šè‰ç¨¿ï¼ˆç­‰å¾… Week1 å®Œæˆåè¡¥å……å®æµ‹ç¬”è®°ï¼‰

---

## âš ï¸ å¼€å§‹å‰å¿…è¯»

**åœ¨å¼€å§‹Week 2ä¹‹å‰ï¼Œè¯·ç¡®è®¤**ï¼š
- âœ… Week 1å·²å®Œæˆï¼ˆ`data/processed/integrated_data.csv` å·²ç”Ÿæˆï¼‰
- âœ… å·²é˜…è¯» `ARCHITECTURE.md` çš„Week 2éƒ¨åˆ†
- âœ… Pythonç¯å¢ƒå·²å®‰è£…å¿…è¦çš„åº“ï¼ˆè§ä¸‹æ–¹ä¾èµ–æ¸…å•ï¼‰

---

## ğŸ¯ æœ¬å‘¨å­¦ä¹ ç›®æ ‡

é€šè¿‡Week 2ï¼Œä½ å°†å­¦ä¼šï¼š

1. **ç‰¹å¾å·¥ç¨‹**ï¼šä»åŸå§‹æ•°æ®åˆ›å»ºé¢„æµ‹ç‰¹å¾
   - æ»åç‰¹å¾ï¼ˆLag featuresï¼‰ï¼šè¿‡å»7å¤©çš„å¹³å‡å€¼
   - æ—¶é—´ç‰¹å¾ï¼ˆTime featuresï¼‰ï¼šæ˜ŸæœŸå‡ ã€æ˜¯å¦å‘¨æœ«
   - åˆ†ç±»ç¼–ç ï¼ˆCategorical encodingï¼‰ï¼šå¹³å°one-hotç¼–ç 

2. **æ¨¡å‹è®­ç»ƒå’Œå¯¹æ¯”**ï¼š
   - æ¨¡å‹1ï¼šçº¿æ€§å›å½’ï¼ˆBaselineï¼‰
   - æ¨¡å‹2ï¼šéšæœºæ£®æ—ï¼ˆRandom Forestï¼‰
   - æ¨¡å‹3ï¼šXGBoostï¼ˆæ¢¯åº¦æå‡æ ‘ï¼‰

3. **æ¨¡å‹è¯„ä¼°**ï¼š
   - MAEï¼ˆMean Absolute Errorï¼‰ï¼šå¹³å‡ç»å¯¹è¯¯å·®
   - RMSEï¼ˆRoot Mean Squared Errorï¼‰ï¼šå‡æ–¹æ ¹è¯¯å·®
   - RÂ²ï¼ˆR-squaredï¼‰ï¼šå†³å®šç³»æ•°

4. **Feature Importanceåˆ†æ**ï¼š
   - å“ªäº›ç‰¹å¾å¯¹ROASé¢„æµ‹æœ€é‡è¦ï¼Ÿ
   - å¦‚ä½•å‘ä¸šåŠ¡æ–¹è§£é‡Šæ¨¡å‹ï¼Ÿ

---

## ğŸ“¦ ä¾èµ–æ¸…å•

åœ¨å¼€å§‹å‰ï¼Œç¡®ä¿å®‰è£…ä»¥ä¸‹Pythonåº“ï¼š

```bash
pip install pandas numpy scikit-learn xgboost matplotlib seaborn
```

**ç‰ˆæœ¬è¦æ±‚**ï¼š
- pandas >= 2.0.3
- numpy >= 1.24.3
- scikit-learn >= 1.3.0
- xgboost >= 1.7.6
- matplotlib >= 3.7.2
- seaborn >= 0.12.2

---

## ğŸš€ Day 2-3 å·¥ä½œæµç¨‹

### Day 2: ç‰¹å¾å·¥ç¨‹ + æ¨¡å‹è®­ç»ƒï¼ˆ4-5å°æ—¶ï¼‰

```
08:00-09:00: æ•°æ®å‡†å¤‡å’Œæ¢ç´¢æ€§åˆ†æ
09:00-11:00: ç‰¹å¾å·¥ç¨‹ï¼ˆæ»åç‰¹å¾ã€æ—¶é—´ç‰¹å¾ï¼‰
11:00-12:00: è®­ç»ƒåŸºå‡†æ¨¡å‹ï¼ˆçº¿æ€§å›å½’ï¼‰
12:00-13:00: åˆä¼‘
13:00-15:00: è®­ç»ƒéšæœºæ£®æ—å’ŒXGBoost
15:00-16:00: æ¨¡å‹å¯¹æ¯”å’Œè¯„ä¼°
```

### Day 3: æ¨¡å‹ä¼˜åŒ– + Feature Importanceï¼ˆ3-4å°æ—¶ï¼‰

```
08:00-10:00: ç‰¹å¾é‡è¦æ€§åˆ†æ
10:00-11:30: æ¨¡å‹è°ƒä¼˜ï¼ˆè¶…å‚æ•°è°ƒæ•´ï¼‰
11:30-12:00: ä¿å­˜æœ€ä½³æ¨¡å‹
```

---

## ğŸ“ Step 1: æ•°æ®å‡†å¤‡å’Œæ¢ç´¢ï¼ˆ30åˆ†é’Ÿï¼‰

### åˆ›å»ºæ–°çš„Notebook

åœ¨ `notebooks/` ç›®å½•ä¸‹åˆ›å»º `02_roas_prediction.ipynb`

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# è®¾ç½®ç»˜å›¾é£æ ¼
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

# è¯»å–Week 1æ¸…æ´—åçš„æ•°æ®
df = pd.read_csv('../data/processed/integrated_data.csv')

print("æ•°æ®æ¦‚è§ˆ:")
print(f"æ€»è¡Œæ•°: {len(df)}")
print(f"åˆ—å: {df.columns.tolist()}")
print(f"\nå‰5è¡Œ:")
print(df.head())
```

### æ¢ç´¢æ€§åˆ†æï¼ˆä½ è¦åšçš„ï¼‰

**ä»»åŠ¡1**: æ£€æŸ¥æ•°æ®ç±»å‹å’Œç¼ºå¤±å€¼

```python
# ä½ çš„ä»£ç 
# æç¤ºï¼šä½¿ç”¨ df.info() å’Œ df.isnull().sum()
```

**ä»»åŠ¡2**: è®¡ç®—åŸºæœ¬ç»Ÿè®¡é‡

```python
# ä½ çš„ä»£ç 
# æç¤ºï¼šä½¿ç”¨ df.describe() æŸ¥çœ‹spend, ROASç­‰æŒ‡æ ‡çš„åˆ†å¸ƒ
```

**ä»»åŠ¡3**: å¯è§†åŒ–ROASåˆ†å¸ƒ

```python
# ä½ çš„ä»£ç 
# æç¤ºï¼šä½¿ç”¨ plt.hist() æˆ– sns.histplot() ç»˜åˆ¶ROASç›´æ–¹å›¾
```

**å¼•å¯¼é—®é¢˜**ï¼š
- â“ ROASçš„å¹³å‡å€¼æ˜¯å¤šå°‘ï¼Ÿä¸­ä½æ•°æ˜¯å¤šå°‘ï¼Ÿ
- â“ ä¸‰ä¸ªå¹³å°çš„ROASåˆ†å¸ƒæœ‰å·®å¼‚å—ï¼Ÿ
- â“ æœ‰æ²¡æœ‰å¼‚å¸¸å€¼ï¼ˆå¦‚ROAS > 10æˆ–ROAS < 0ï¼‰ï¼Ÿ

---

## ğŸ“ Step 2: ç‰¹å¾å·¥ç¨‹ï¼ˆ2-3å°æ—¶ï¼‰

### 2.1 è½¬æ¢æ—¥æœŸåˆ—

é¦–å…ˆï¼Œç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹ï¼š

```python
# è½¬æ¢æ—¥æœŸåˆ—
df['date'] = pd.to_datetime(df['date'])

# æŒ‰æ—¥æœŸæ’åºï¼ˆé‡è¦ï¼æ»åç‰¹å¾éœ€è¦é¡ºåºï¼‰
df = df.sort_values(['platform', 'date']).reset_index(drop=True)

print("æ—¥æœŸèŒƒå›´:", df['date'].min(), "è‡³", df['date'].max())
```

### 2.2 åˆ›å»ºæ»åç‰¹å¾ï¼ˆLag Featuresï¼‰

**æ ¸å¿ƒæ€æƒ³**: ä½¿ç”¨è¿‡å»7å¤©çš„æ•°æ®é¢„æµ‹æœªæ¥ROAS

```python
# ç¤ºä¾‹ä»£ç ï¼ˆä½ éœ€è¦å®Œæˆï¼‰
def create_lag_features(df, window=7):
    """
    åˆ›å»ºæ»åç‰¹å¾

    å‚æ•°:
        df: åŸå§‹æ•°æ®
        window: æ»šåŠ¨çª—å£å¤§å°ï¼ˆå¤©æ•°ï¼‰

    è¿”å›:
        å¸¦æ»åç‰¹å¾çš„æ•°æ®
    """
    df_features = df.copy()

    # æŒ‰å¹³å°åˆ†ç»„ï¼Œè®¡ç®—è¿‡å»7å¤©çš„å¹³å‡å€¼
    for platform in ['Meta', 'Google', 'TikTok']:
        platform_data = df_features[df_features['platform'] == platform]

        # æ»åç‰¹å¾ï¼šè¿‡å»7å¤©çš„å¹³å‡spend
        # ä½ çš„ä»£ç ï¼šä½¿ç”¨ .rolling(window).mean()

        # æ»åç‰¹å¾ï¼šè¿‡å»7å¤©çš„å¹³å‡ctr
        # ä½ çš„ä»£ç 

        # æ»åç‰¹å¾ï¼šè¿‡å»7å¤©çš„å¹³å‡cvr
        # ä½ çš„ä»£ç 

        # æ»åç‰¹å¾ï¼šè¿‡å»7å¤©çš„å¹³å‡cpa
        # ä½ çš„ä»£ç 

    return df_features
```

**å¼•å¯¼é—®é¢˜**ï¼š
- â“ ä¸ºä»€ä¹ˆé€‰æ‹©7å¤©çª—å£ï¼Ÿä¸æ˜¯3å¤©æˆ–14å¤©ï¼Ÿ
- â“ `.rolling(7).mean()` çš„ç¬¬1-6è¡Œä¼šæ˜¯ä»€ä¹ˆï¼Ÿï¼ˆæç¤ºï¼šä¼šæœ‰NaNï¼‰
- â“ å¦‚ä½•å¤„ç†è¿™äº›NaNè¡Œï¼Ÿåˆ é™¤è¿˜æ˜¯å¡«å……ï¼Ÿ

### 2.3 åˆ›å»ºæ—¶é—´ç‰¹å¾ï¼ˆTime Featuresï¼‰

```python
# ä½ çš„ä»£ç 
# ä»»åŠ¡1: åˆ›å»ºday_of_weekç‰¹å¾ï¼ˆ0=å‘¨ä¸€, 6=å‘¨æ—¥ï¼‰
# æç¤ºï¼šä½¿ç”¨ df['date'].dt.dayofweek

# ä»»åŠ¡2: åˆ›å»ºis_weekendç‰¹å¾ï¼ˆ0=å·¥ä½œæ—¥, 1=å‘¨æœ«ï¼‰
# æç¤ºï¼šday_of_week >= 5 å³ä¸ºå‘¨æœ«

# ä»»åŠ¡3: åˆ›å»ºmonthç‰¹å¾ï¼ˆ1-12æœˆï¼‰
# æç¤ºï¼šä½¿ç”¨ df['date'].dt.month
```

### 2.4 åˆ†ç±»ç¼–ç ï¼ˆCategorical Encodingï¼‰

```python
# ä»»åŠ¡ï¼šå°†platformåˆ—è½¬æ¢ä¸ºone-hotç¼–ç 
# ä½ çš„ä»£ç 
# æç¤ºï¼šä½¿ç”¨ pd.get_dummies(df, columns=['platform'])
```

**å¼•å¯¼é—®é¢˜**ï¼š
- â“ ä¸ºä»€ä¹ˆè¦åšone-hotç¼–ç ï¼Ÿæ¨¡å‹ä¸èƒ½ç›´æ¥ç”¨"Meta"ã€"Google"å­—ç¬¦ä¸²å—ï¼Ÿ
- â“ one-hotç¼–ç åä¼šç”Ÿæˆå‡ åˆ—ï¼Ÿ
- â“ å¦‚æœæœ‰100ä¸ªä¸åŒçš„campaign_nameï¼Œè¦ä¸è¦ä¹Ÿåšone-hotï¼Ÿ

### 2.5 é€‰æ‹©æœ€ç»ˆç‰¹å¾å’Œç›®æ ‡å˜é‡

```python
# å®šä¹‰ç‰¹å¾åˆ—ï¼ˆXï¼‰å’Œç›®æ ‡åˆ—ï¼ˆyï¼‰
feature_columns = [
    'spend_last_7d',
    'ctr_last_7d',
    'cvr_last_7d',
    'cpa_last_7d',
    'day_of_week',
    'is_weekend',
    'month',
    'platform_Meta',
    'platform_Google',
    'platform_TikTok'
]

X = df_features[feature_columns]
y = df_features['roas']

print("ç‰¹å¾çŸ©é˜µå½¢çŠ¶:", X.shape)
print("ç›®æ ‡å˜é‡å½¢çŠ¶:", y.shape)
print(f"\nåˆ é™¤NaNåè¡Œæ•°: {X.dropna().shape[0]}")
```

---

## ğŸ“ Step 3: åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆ30åˆ†é’Ÿï¼‰

### 3.1 æ—¶é—´åºåˆ—åˆ’åˆ†ï¼ˆé‡è¦ï¼ï¼‰

âš ï¸ **å…³é”®åŸåˆ™**: å¹¿å‘Šæ•°æ®æ˜¯æ—¶é—´åºåˆ—ï¼Œä¸èƒ½éšæœºåˆ’åˆ†ï¼

```python
from sklearn.model_selection import train_test_split

# âŒ é”™è¯¯åšæ³•ï¼ˆéšæœºåˆ’åˆ†ï¼‰
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# âœ… æ­£ç¡®åšæ³•ï¼ˆæ—¶é—´åˆ’åˆ†ï¼‰
# å‰80%ä½œä¸ºè®­ç»ƒé›†ï¼Œå20%ä½œä¸ºæµ‹è¯•é›†
split_index = int(len(X) * 0.8)

X_train = X[:split_index]
X_test = X[split_index:]
y_train = y[:split_index]
y_test = y[split_index:]

print(f"è®­ç»ƒé›†: {len(X_train)}è¡Œ")
print(f"æµ‹è¯•é›†: {len(X_test)}è¡Œ")
print(f"è®­ç»ƒé›†æ—¥æœŸèŒƒå›´: {df.loc[X_train.index, 'date'].min()} è‡³ {df.loc[X_train.index, 'date'].max()}")
print(f"æµ‹è¯•é›†æ—¥æœŸèŒƒå›´: {df.loc[X_test.index, 'date'].min()} è‡³ {df.loc[X_test.index, 'date'].max()}")
```

**å¼•å¯¼é—®é¢˜**ï¼š
- â“ ä¸ºä»€ä¹ˆä¸èƒ½éšæœºåˆ’åˆ†æ—¶é—´åºåˆ—æ•°æ®ï¼Ÿ
- â“ å¦‚æœéšæœºåˆ’åˆ†ï¼Œä¼šå¯¼è‡´ä»€ä¹ˆé—®é¢˜ï¼Ÿï¼ˆæç¤ºï¼šç”¨æœªæ¥æ•°æ®é¢„æµ‹è¿‡å»ï¼‰
- â“ 80/20åˆ’åˆ†æ˜¯å›ºå®šçš„å—ï¼Ÿå¯ä»¥æ”¹æˆ70/30å—ï¼Ÿ

---

## ğŸ“ Step 4: è®­ç»ƒåŸºå‡†æ¨¡å‹ï¼ˆçº¿æ€§å›å½’ï¼‰ï¼ˆ30åˆ†é’Ÿï¼‰

### 4.1 è®­ç»ƒçº¿æ€§å›å½’

```python
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# è®­ç»ƒæ¨¡å‹
lr = LinearRegression()
lr.fit(X_train, y_train)

# é¢„æµ‹
lr_pred = lr.predict(X_test)

# è¯„ä¼°
lr_mae = mean_absolute_error(y_test, lr_pred)
lr_rmse = np.sqrt(mean_squared_error(y_test, lr_pred))
lr_r2 = r2_score(y_test, lr_pred)

print("çº¿æ€§å›å½’æ¨¡å‹è¯„ä¼°:")
print(f"MAE: {lr_mae:.4f}")
print(f"RMSE: {lr_rmse:.4f}")
print(f"RÂ²: {lr_r2:.4f}")
```

### 4.2 å¯è§†åŒ–é¢„æµ‹ç»“æœ

```python
# ä½ çš„ä»£ç 
# ä»»åŠ¡ï¼šç»˜åˆ¶æ•£ç‚¹å›¾ï¼Œæ¨ªè½´æ˜¯çœŸå®ROASï¼Œçºµè½´æ˜¯é¢„æµ‹ROAS
# æç¤ºï¼šä½¿ç”¨ plt.scatter(y_test, lr_pred)
# æ·»åŠ å¯¹è§’çº¿ï¼ˆy=xï¼‰ï¼Œè¡¨ç¤ºå®Œç¾é¢„æµ‹
# æç¤ºï¼šplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
```

**å¼•å¯¼é—®é¢˜**ï¼š
- â“ MAE=0.52æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿï¼ˆæç¤ºï¼šå¹³å‡é¢„æµ‹è¯¯å·®0.52ç¾å…ƒROASï¼‰
- â“ RÂ²=0.65æ˜¯å¥½è¿˜æ˜¯åï¼Ÿï¼ˆæç¤ºï¼š65%çš„æ–¹å·®è¢«è§£é‡Šï¼‰
- â“ ä¸ºä»€ä¹ˆæ•£ç‚¹å›¾ä¸­æœ‰äº›ç‚¹ç¦»å¯¹è§’çº¿å¾ˆè¿œï¼Ÿ

---

## ğŸ“ Step 5: è®­ç»ƒéšæœºæ£®æ—ï¼ˆ2å°æ—¶ï¼‰

### 5.1 è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹

```python
from sklearn.ensemble import RandomForestRegressor

# è®­ç»ƒæ¨¡å‹
rf = RandomForestRegressor(
    n_estimators=100,      # 100æ£µæ ‘
    max_depth=10,          # æœ€å¤§æ·±åº¦10
    min_samples_split=5,   # èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°5
    random_state=42
)

rf.fit(X_train, y_train)

# é¢„æµ‹
rf_pred = rf.predict(X_test)

# è¯„ä¼°
rf_mae = mean_absolute_error(y_test, rf_pred)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))
rf_r2 = r2_score(y_test, rf_pred)

print("éšæœºæ£®æ—æ¨¡å‹è¯„ä¼°:")
print(f"MAE: {rf_mae:.4f}")
print(f"RMSE: {rf_rmse:.4f}")
print(f"RÂ²: {rf_r2:.4f}")
```

### 5.2 Feature Importanceåˆ†æ

```python
# æå–ç‰¹å¾é‡è¦æ€§
feature_importance = pd.DataFrame({
    'feature': feature_columns,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)

print("ç‰¹å¾é‡è¦æ€§æ’å:")
print(feature_importance)

# å¯è§†åŒ–
plt.figure(figsize=(10, 6))
plt.barh(feature_importance['feature'][:10], feature_importance['importance'][:10])
plt.xlabel('Importance')
plt.title('Top 10 Feature Importance (Random Forest)')
plt.tight_layout()
plt.savefig('../output/figures/feature_importance.png', dpi=300)
plt.show()
```

**å¼•å¯¼é—®é¢˜**ï¼š
- â“ å“ªä¸ªç‰¹å¾æœ€é‡è¦ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
- â“ æ—¶é—´ç‰¹å¾ï¼ˆis_weekendï¼‰é‡è¦å—ï¼Ÿ
- â“ ä¸‰ä¸ªå¹³å°çš„one-hotç‰¹å¾é‡è¦æ€§æœ‰å·®å¼‚å—ï¼Ÿ

---

## ğŸ“ Step 6: è®­ç»ƒXGBoostï¼ˆ1å°æ—¶ï¼‰

### 6.1 è®­ç»ƒXGBoostæ¨¡å‹

```python
from xgboost import XGBRegressor

# è®­ç»ƒæ¨¡å‹
xgb = XGBRegressor(
    n_estimators=100,
    max_depth=6,
    learning_rate=0.1,
    random_state=42
)

xgb.fit(X_train, y_train)

# é¢„æµ‹
xgb_pred = xgb.predict(X_test)

# è¯„ä¼°
xgb_mae = mean_absolute_error(y_test, xgb_pred)
xgb_rmse = np.sqrt(mean_squared_error(y_test, xgb_pred))
xgb_r2 = r2_score(y_test, xgb_pred)

print("XGBoostæ¨¡å‹è¯„ä¼°:")
print(f"MAE: {xgb_mae:.4f}")
print(f"RMSE: {xgb_rmse:.4f}")
print(f"RÂ²: {xgb_r2:.4f}")
```

---

## ğŸ“ Step 7: æ¨¡å‹å¯¹æ¯”å’Œé€‰æ‹©ï¼ˆ1å°æ—¶ï¼‰

### 7.1 å¯¹æ¯”ä¸‰ä¸ªæ¨¡å‹

```python
# ä½ çš„ä»£ç 
# ä»»åŠ¡ï¼šåˆ›å»ºä¸€ä¸ªDataFrameï¼Œå¯¹æ¯”ä¸‰ä¸ªæ¨¡å‹çš„MAEã€RMSEã€RÂ²
# æç¤ºï¼š
results = pd.DataFrame({
    'Model': ['Linear Regression', 'Random Forest', 'XGBoost'],
    'MAE': [lr_mae, rf_mae, xgb_mae],
    'RMSE': [lr_rmse, rf_rmse, xgb_rmse],
    'RÂ²': [lr_r2, rf_r2, xgb_r2]
})

print(results)
```

### 7.2 å¯è§†åŒ–å¯¹æ¯”

```python
# ä½ çš„ä»£ç 
# ä»»åŠ¡ï¼šç»˜åˆ¶æŸ±çŠ¶å›¾ï¼Œå¯¹æ¯”ä¸‰ä¸ªæ¨¡å‹çš„MAE
# æç¤ºï¼š
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# å­å›¾1ï¼šMAEå¯¹æ¯”
axes[0].bar(results['Model'], results['MAE'])
axes[0].set_title('MAE Comparison')
axes[0].set_ylabel('MAE')

# å­å›¾2ï¼šRMSEå¯¹æ¯”
# ä½ çš„ä»£ç 

# å­å›¾3ï¼šRÂ²å¯¹æ¯”
# ä½ çš„ä»£ç 

plt.tight_layout()
plt.savefig('../output/figures/model_comparison.png', dpi=300)
plt.show()
```

### 7.3 é€‰æ‹©æœ€ä½³æ¨¡å‹

**å†³ç­–åŸåˆ™**ï¼š
1. **é¦–å…ˆçœ‹MAE**ï¼ˆæœ€ç›´è§‚çš„è¯¯å·®æŒ‡æ ‡ï¼‰
2. **è€ƒè™‘RÂ²**ï¼ˆè§£é‡Šèƒ½åŠ›ï¼‰
3. **è€ƒè™‘è®­ç»ƒæ—¶é—´**ï¼ˆç”Ÿäº§ç¯å¢ƒé‡è¦ï¼‰
4. **è€ƒè™‘å¯è§£é‡Šæ€§**ï¼ˆå‘ä¸šåŠ¡æ–¹è§£é‡Šï¼‰

**å¼•å¯¼é—®é¢˜**ï¼š
- â“ å“ªä¸ªæ¨¡å‹çš„MAEæœ€ä½ï¼Ÿ
- â“ å¦‚æœXGBoostçš„MAEæ¯”éšæœºæ£®æ—ä½0.03ï¼Œä½†è®­ç»ƒæ—¶é—´å¤š3å€ï¼Œä½ ä¼šé€‰å“ªä¸ªï¼Ÿ
- â“ å¦‚ä½•å‘Marketing Directorè§£é‡Š"ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæ¨¡å‹"ï¼Ÿ

---

## ğŸ“ Step 8: ä¿å­˜æ¨¡å‹å’Œç»“æœï¼ˆ30åˆ†é’Ÿï¼‰

### 8.1 ä¿å­˜æœ€ä½³æ¨¡å‹

```python
import pickle

# å‡è®¾é€‰æ‹©éšæœºæ£®æ—ä½œä¸ºæœ€ä½³æ¨¡å‹
best_model = rf
best_model_name = 'random_forest'

# ä¿å­˜æ¨¡å‹
with open(f'../output/models/{best_model_name}_model.pkl', 'wb') as f:
    pickle.dump(best_model, f)

print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° output/models/{best_model_name}_model.pkl")
```

### 8.2 ä¿å­˜æ¨¡å‹è¯„ä¼°æŒ‡æ ‡

```python
import json

# ä¿å­˜æŒ‡æ ‡
metrics = {
    'model_name': best_model_name,
    'mae': float(rf_mae),
    'rmse': float(rf_rmse),
    'r2': float(rf_r2),
    'train_size': len(X_train),
    'test_size': len(X_test),
    'feature_columns': feature_columns,
    'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
}

with open('../output/models/model_metrics.json', 'w') as f:
    json.dump(metrics, f, indent=2)

print("âœ… æ¨¡å‹æŒ‡æ ‡å·²ä¿å­˜åˆ° output/models/model_metrics.json")
```

### 8.3 ç”Ÿæˆæ¨¡å‹æŠ¥å‘Š

```python
# ä½ çš„ä»£ç 
# ä»»åŠ¡ï¼šåœ¨notebookæœ«å°¾æ·»åŠ ä¸€ä¸ªMarkdownå•å…ƒæ ¼ï¼Œæ€»ç»“ï¼š
# - æœ€ä½³æ¨¡å‹æ˜¯ä»€ä¹ˆ
# - MAE/RMSE/RÂ²æŒ‡æ ‡
# - å‰5ä¸ªæœ€é‡è¦çš„ç‰¹å¾
# - æ¨¡å‹çš„å±€é™æ€§ï¼ˆå¦‚æ•°æ®é‡å°ã€æ—¶é—´èŒƒå›´çŸ­ï¼‰
# - æœªæ¥æ”¹è¿›æ–¹å‘ï¼ˆå¦‚å¢åŠ æ›´å¤šç‰¹å¾ã€è°ƒä¼˜è¶…å‚æ•°ï¼‰
```

---

## ğŸ“ ä½ å°†å­¦åˆ°ä»€ä¹ˆ

é€šè¿‡Week 2ï¼Œä½ å°†æŒæ¡ï¼š

### æŠ€æœ¯èƒ½åŠ›
- âœ… ç‰¹å¾å·¥ç¨‹ï¼ˆæ»åç‰¹å¾ã€æ—¶é—´ç‰¹å¾ã€åˆ†ç±»ç¼–ç ï¼‰
- âœ… æ—¶é—´åºåˆ—åˆ’åˆ†ï¼ˆä¸èƒ½éšæœºï¼ï¼‰
- âœ… è®­ç»ƒå¤šä¸ªæ¨¡å‹ï¼ˆçº¿æ€§å›å½’ã€éšæœºæ£®æ—ã€XGBoostï¼‰
- âœ… æ¨¡å‹è¯„ä¼°ï¼ˆMAEã€RMSEã€RÂ²ï¼‰
- âœ… Feature Importanceåˆ†æ
- âœ… æ¨¡å‹æŒä¹…åŒ–ï¼ˆpickleï¼‰

### ä¸šåŠ¡ç†è§£
- âœ… ä¸ºä»€ä¹ˆROASé¢„æµ‹é‡è¦ï¼Ÿï¼ˆé¢„ç®—è§„åˆ’ã€æ•ˆæœè¯„ä¼°ï¼‰
- âœ… å¦‚ä½•é€‰æ‹©æ¨¡å‹ï¼Ÿï¼ˆå‡†ç¡®æ€§ vs é€Ÿåº¦ vs å¯è§£é‡Šæ€§ï¼‰
- âœ… å¦‚ä½•å‘éæŠ€æœ¯äººå‘˜è§£é‡Šæ¨¡å‹ï¼Ÿ

### é¢è¯•å‡†å¤‡
- âœ… èƒ½æµç•…è®²è¿°æ¨¡å‹è®­ç»ƒè¿‡ç¨‹
- âœ… èƒ½è§£é‡Šä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªæ¨¡å‹
- âœ… èƒ½å›ç­”"MAE=0.42æ˜¯å¥½è¿˜æ˜¯å"
- âœ… èƒ½è®¨è®ºæ¨¡å‹çš„å±€é™æ€§å’Œæ”¹è¿›æ–¹å‘

---

## âœ… Week 2 å®Œæˆæ ‡å‡†

å®Œæˆä»¥ä¸‹æ‰€æœ‰ä»»åŠ¡ï¼ŒWeek 2å°±ç®—å®Œæˆï¼š

1. âœ… åˆ›å»ºäº† `notebooks/02_roas_prediction.ipynb`
2. âœ… å®Œæˆäº†ç‰¹å¾å·¥ç¨‹ï¼ˆæ»åç‰¹å¾ + æ—¶é—´ç‰¹å¾ï¼‰
3. âœ… è®­ç»ƒäº†3ä¸ªæ¨¡å‹ï¼ˆçº¿æ€§å›å½’ã€éšæœºæ£®æ—ã€XGBoostï¼‰
4. âœ… ç”Ÿæˆäº†æ¨¡å‹å¯¹æ¯”å›¾è¡¨
5. âœ… åˆ†æäº†Feature Importance
6. âœ… ä¿å­˜äº†æœ€ä½³æ¨¡å‹ï¼ˆ.pklæ–‡ä»¶ï¼‰
7. âœ… ä¿å­˜äº†æ¨¡å‹æŒ‡æ ‡ï¼ˆ.jsonæ–‡ä»¶ï¼‰
8. âœ… ç”Ÿæˆäº†é«˜è´¨é‡çš„å›¾è¡¨ï¼ˆfeature_importance.png, model_comparison.pngï¼‰

---

## ğŸš€ ä¸‹ä¸€æ­¥

å®ŒæˆWeek 2åï¼Œå‘Šè¯‰æˆ‘ï¼š

**"Week 2å®Œæˆäº†"**

ç„¶åæˆ‘ä»¬å¼€å§‹ï¼š
- **Week 2ï¼ˆä¸‹åŠéƒ¨åˆ†ï¼‰**: é¢„ç®—ä¼˜åŒ–ï¼ˆä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼‰
- **Week 3**: A/Bæµ‹è¯•åˆ†æ + è‡ªåŠ¨åŒ–ETL

è®°ä½ï¼š**ä¸è¦æ€¥ç€å†™ä»£ç ï¼Œå…ˆç†è§£åŸç†ï¼**

---

**åˆ›å»ºæ—¶é—´**: 2025-10-11
**ç›®çš„**: å¼•å¯¼ä½ ä¸€æ­¥æ­¥å­¦ä¹ æœºå™¨å­¦ä¹ å»ºæ¨¡ï¼Œä»ç‰¹å¾å·¥ç¨‹åˆ°æ¨¡å‹è¯„ä¼°
