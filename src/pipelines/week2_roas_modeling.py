"""
Week 2 ROAS modeling pipeline.

This module turns the exploratory notebook logic into reusable functions that:
1. prepare daily, platform-level features with lagged context, and
2. train a residual Random Forest model to predict ROAS uplift beyond the
   trailing 7-day average.
"""

from __future__ import annotations

from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit
import json


# Holidays used in the original notebook feature engineering
HOLIDAYS_2024 = pd.to_datetime(
    ["2024-01-01", "2024-02-14", "2024-03-17", "2024-07-04", "2024-11-28"]
)


@dataclass
class ModelArtifacts:
    """Paths generated by run_week2_pipeline."""

    model_path: Path
    metrics_path: Path


def prepare_daily_features(
    integrated_path: Path,
    lag_days: int = 7,
) -> pd.DataFrame:
    """
    Aggregate integrated platform data to a modeling-ready dataframe.

    The function mirrors the feature engineering steps from the notebook but
    removes exploratory prints/side effects.
    """
    df = pd.read_csv(integrated_path)
    df["date"] = pd.to_datetime(df["date"])

    daily = (
        df.groupby(["date", "platform"], as_index=False)
        .agg(
            spend=("spend", "sum"),
            revenue=("revenue", "sum"),
            clicks=("clicks", "sum"),
            conversions=("conversions", "sum"),
            impressions=("impressions", "sum"),
        )
        .sort_values(["date", "platform"])
        .reset_index(drop=True)
    )

    daily["roas"] = daily["revenue"] / daily["spend"]
    daily["ctr"] = daily["clicks"] / daily["impressions"]
    daily["cvr"] = daily["conversions"] / daily["clicks"]
    daily["cpa"] = daily["spend"] / daily["conversions"]
    daily[["roas", "ctr", "cvr", "cpa"]] = daily[["roas", "ctr", "cvr", "cpa"]].replace(
        [np.inf, -np.inf], np.nan
    )

    # Fill unavoidable NaNs (mostly from zero conversions/clicks)
    daily["roas"] = daily["roas"].fillna(0.0)
    daily["cvr"] = daily["cvr"].fillna(0.0)
    daily["cpa"] = daily["cpa"].fillna(daily["cpa"].median())
    daily["ctr"] = daily["ctr"].fillna(0.0)

    lag_features = ["roas", "spend", "ctr", "cvr"]
    for col in lag_features:
        daily[f"{col}_last_{lag_days}"] = (
            daily.groupby("platform")[col]
            .transform(lambda s: s.rolling(window=lag_days, min_periods=lag_days).mean())
        )

    daily["residual"] = daily["roas"] - daily[f"roas_last_{lag_days}"]

    # Growth features
    daily["spend_growth"] = (
        daily.groupby("platform")["spend"].pct_change().replace([np.inf, -np.inf], 0.0)
    )
    daily["conv_growth"] = (
        daily.groupby("platform")["conversions"]
        .pct_change()
        .replace([np.inf, -np.inf], 0.0)
    )
    daily[["spend_growth", "conv_growth"]] = daily[
        ["spend_growth", "conv_growth"]
    ].fillna(0.0)

    # Calendar features
    daily["month"] = daily["date"].dt.month
    daily["day_of_week"] = daily["date"].dt.dayofweek
    daily["is_weekend"] = (daily["day_of_week"] >= 5).astype(int)
    daily["is_q4"] = daily["month"].isin([10, 11, 12]).astype(int)
    daily["is_holiday"] = daily["date"].isin(HOLIDAYS_2024).astype(int)

    month_dummies = pd.get_dummies(daily["month"], prefix="month", drop_first=False)
    platform_dummies = pd.get_dummies(daily["platform"], prefix="platform")

    feature_df = pd.concat(
        [daily, month_dummies, platform_dummies],
        axis=1,
    )

    # Drop rows without enough history for lag features.
    feature_df = feature_df.dropna(subset=[f"roas_last_{lag_days}"]).reset_index(
        drop=True
    )

    return feature_df


def build_feature_matrix(
    feature_df: pd.DataFrame,
    lag_days: int = 7,
) -> Tuple[pd.DataFrame, pd.Series, pd.Series]:
    """
    Construct X and y matrices for modeling.

    Returns
    -------
    X : DataFrame of features.
    y_residual : Series of residual targets (roas - rolling mean).
    roas_last : Series of trailing roas to reconstruct absolute predictions later.
    """
    exclude_cols = {
        "revenue",
        "clicks",
        "conversions",
        "impressions",
        "date",
        "platform",
        "roas",
    }
    residual_col = f"roas_last_{lag_days}"

    base_cols = [
        col
        for col in feature_df.columns
        if col not in exclude_cols and not col.startswith("month_")
        and not col.startswith("platform_")
    ]
    month_cols = sorted([col for col in feature_df.columns if col.startswith("month_")])
    platform_cols = sorted(
        [col for col in feature_df.columns if col.startswith("platform_")]
    )

    feature_cols = base_cols + month_cols + platform_cols
    feature_cols = [col for col in feature_cols if col != "residual"]

    X = feature_df[feature_cols].copy()
    y_residual = feature_df["residual"].copy()
    roas_last = feature_df[residual_col].copy()
    return X, y_residual, roas_last


def time_series_split_masks(
    dates: pd.Series,
    test_size: float = 0.2,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Create boolean masks for time-based train/test split.

    Parameters
    ----------
    dates: pd.Series
        Series of datetime values aligned with the feature matrix.
    test_size: float
        Fraction of examples to reserve for testing (chronological split).
    """
    sorted_dates = dates.sort_values().unique()
    cutoff_index = int(len(sorted_dates) * (1 - test_size))
    cutoff_date = sorted_dates[cutoff_index]

    train_mask = dates <= cutoff_date
    test_mask = dates > cutoff_date
    return train_mask.values, test_mask.values


def train_residual_random_forest(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    search_iterations: int = 12,
    random_state: int = 42,
) -> RandomForestRegressor:
    """
    Tune and fit a RandomForestRegressor on residuals.
    """
    rf = RandomForestRegressor(
        bootstrap=True,
        random_state=random_state,
        n_jobs=-1,
    )
    param_grid = {
        "n_estimators": [300, 500, 700],
        "max_depth": [6, 8, 10],
        "min_samples_leaf": [3, 5, 10],
        "min_samples_split": [8, 12, 16],
        "max_features": [0.6, 0.8, "sqrt"],
    }
    search = RandomizedSearchCV(
        rf,
        param_grid,
        n_iter=search_iterations,
        n_jobs=-1,
        cv=TimeSeriesSplit(n_splits=3),
        scoring="neg_mean_absolute_error",
        random_state=random_state,
        verbose=0,
    )
    search.fit(X_train, y_train)
    return search.best_estimator_


def evaluate_predictions(
    y_true: pd.Series,
    y_pred: np.ndarray,
) -> Dict[str, float]:
    """Return MAE, RMSE, and RÂ² metrics."""
    return {
        "mae": float(mean_absolute_error(y_true, y_pred)),
        "rmse": float(mean_squared_error(y_true, y_pred) ** 0.5),
        "r2": float(r2_score(y_true, y_pred)),
    }


def run_week2_pipeline(
    integrated_path: Path,
    models_dir: Path,
    metrics_dir: Optional[Path] = None,
    test_size: float = 0.2,
    lag_days: int = 7,
) -> ModelArtifacts:
    """
    Execute the Week 2 modeling workflow end-to-end.
    """
    models_dir.mkdir(parents=True, exist_ok=True)
    if metrics_dir:
        metrics_dir.mkdir(parents=True, exist_ok=True)
    else:
        metrics_dir = models_dir

    feature_df = prepare_daily_features(integrated_path, lag_days=lag_days)
    X, y_residual, roas_last = build_feature_matrix(feature_df, lag_days=lag_days)

    train_mask, test_mask = time_series_split_masks(feature_df["date"], test_size)
    X_train, X_test = X.iloc[train_mask], X.iloc[test_mask]
    y_train, y_test = y_residual.iloc[train_mask], y_residual.iloc[test_mask]
    roas_train_last = roas_last.iloc[train_mask]
    roas_test_last = roas_last.iloc[test_mask]
    roas_train_actual = feature_df.loc[train_mask, "roas"]
    roas_test_actual = feature_df.loc[test_mask, "roas"]

    model = train_residual_random_forest(X_train, y_train)

    train_pred_resid = model.predict(X_train)
    test_pred_resid = model.predict(X_test)

    train_roas_pred = roas_train_last + train_pred_resid
    test_roas_pred = roas_test_last + test_pred_resid

    metrics = {
        "train": evaluate_predictions(roas_train_actual, train_roas_pred),
        "test": evaluate_predictions(roas_test_actual, test_roas_pred),
        "baseline": {
            "mae": float(mean_absolute_error(roas_test_actual, roas_test_last)),
        },
    }

    model_path = models_dir / "random_forest_roas.pkl"
    metrics_path = metrics_dir / "random_forest_roas_metrics.json"

    pd.to_pickle(model, model_path)
    metrics_path.write_text(json.dumps(metrics, indent=2), encoding="utf-8")

    return ModelArtifacts(model_path=model_path, metrics_path=metrics_path)


__all__ = [
    "ModelArtifacts",
    "prepare_daily_features",
    "build_feature_matrix",
    "time_series_split_masks",
    "train_residual_random_forest",
    "evaluate_predictions",
    "run_week2_pipeline",
]
